---
id: RES-004
topic: Cluster Protocol Architecture for Rust Server
created: 2026-02-19
status: complete
blocks: TODO-063 (advanced), TODO-066
---

# Research: Cluster Protocol Architecture for Rust Server

## Summary

This research synthesizes Hazelcast's battle-tested cluster protocol (versioned membership, master-centric coordination, 3-phase migration), Quickwit's Rust-native gossip membership (chitchat + ClusterChangeStream), and TiKV's per-partition FSM concurrency (DashMap + batch system) into a TopGun-specific design. The key insight is that TopGun's CRDT foundation fundamentally simplifies migration and split-brain recovery compared to Hazelcast: CRDTs merge deterministically, so partitions can accept writes during migration and split-brain heals automatically on reconnect. The recommended architecture uses master-centric coordination with gossip-assisted failure detection, DashMap-based partition table, and a simplified 2-phase migration (prepare + finalize, no strict locking thanks to CRDTs).

## Background

The TypeScript ClusterManager (`packages/server/src/cluster/ClusterManager.ts`) is architecturally simplistic:
- No versioned membership views (members are a flat `Map<string, ClusterMember>`)
- No explicit join ceremony (nodes connect via WebSocket and exchange HELLO messages)
- No split-brain detection or recovery
- Simple modulo rebalancing (`partition_id = i % members.length`) with no migration lifecycle
- No partition state machine (partitions have no concept of MIGRATING state internally)

Hazelcast addresses all of these with enterprise-grade protocols, but its Java-centric design (inheritance hierarchies, shared mutable state under locks, GC-aware patterns) does not translate directly to Rust. Meanwhile, Rust OSS projects (Quickwit, TiKV) demonstrate idiomatic patterns for similar problems.

The goal is to design a cluster protocol that:
1. Uses Hazelcast's conceptual architecture (WHAT to build)
2. Implements with Rust-native patterns (HOW to build)
3. Leverages TopGun's CRDT advantage for simpler migration and split-brain recovery
4. Supports 271 fixed partitions with configurable backup count
5. Enables clients to do partition-aware routing (first-class cluster participants)

## Options Explored

### Option 1: Full Hazelcast Port (master-centric, TCP mesh, Java patterns)

**Description:** Directly replicate Hazelcast's architecture: master node coordinates all membership changes, TCP mesh between all nodes, MembersView with monotonic version, MigrationPlanner with 3-phase commit.

**Pros:**
- Proven at scale (Hazelcast handles 1000+ node clusters)
- Well-understood failure modes
- Complete feature set (join ceremony, split-brain, migration ordering)

**Cons:**
- Java patterns don't translate to Rust (inheritance, shared mutable state under locks)
- TCP mesh has O(n^2) connection overhead
- 3-phase migration is heavyweight when CRDTs can merge during migration
- Master election via oldest-member convention requires careful Rust implementation

### Option 2: Gossip-Only (Quickwit chitchat style)

**Description:** Use gossip for all cluster coordination. Membership via scuttlebutt anti-entropy, failure detection via phi-accrual in gossip layer, partition assignment via deterministic algorithm on each node independently.

**Pros:**
- Decentralized, no single point of failure
- Quickwit demonstrates this works well in Rust (chitchat crate)
- Simple implementation, fewer moving parts
- O(log n) message complexity per gossip round

**Cons:**
- Eventual consistency for membership means partition table can be inconsistent across nodes temporarily
- No coordinator means migration ordering is harder
- Split-brain detection requires external mechanism
- Not suitable for coordinated multi-step operations (migration)

### Option 3: Hybrid (master-centric coordination + gossip-assisted failure detection)

**Description:** Master node coordinates membership changes and migration (like Hazelcast), but failure detection uses gossip-style heartbeats (like chitchat). Partition table is centrally managed but distributed via efficient push. CRDTs simplify migration to 2 phases.

**Pros:**
- Best of both worlds: coordinated changes + decentralized health monitoring
- CRDT-aware: migration doesn't need strict locking (concurrent writes merge)
- Partition table consistency guaranteed by master versioning
- Failure detection doesn't depend on master availability
- Natural fit for TopGun's "extend the cluster into the browser" model

**Cons:**
- More complex than pure gossip
- Master is still a coordination bottleneck (mitigated by fast master re-election)
- Two protocols to maintain (coordination + gossip)

## Codebase Findings

### Hazelcast (conceptual architecture)

**MembershipManager** (`hazelcast/internal/cluster/impl/MembershipManager.java`):
- `AtomicReference<MemberMap>` for lock-free reads of current membership
- `MemberMap` is immutable, versioned, dual-indexed by Address and UUID
- `MembersView` is the serializable form: `{version: int, members: List<MemberInfo>}`
- Master publishes member list periodically (`sendMemberListToOthers`)
- Non-master nodes receive `MembersUpdateOp` and atomically swap their `MemberMap`
- Suspected members tracked in `ConcurrentHashMap.newKeySet()`

**ClusterHeartbeatManager** (`hazelcast/internal/cluster/impl/ClusterHeartbeatManager.java`):
- Pluggable failure detection: `ClusterFailureDetector` interface with `heartbeat()`, `isAlive()`, `suspicionLevel()`
- Two implementations: `DeadlineClusterFailureDetector` (simple timeout) and `PhiAccrualClusterFailureDetector`
- Heartbeat complaint protocol: non-master nodes report invalid heartbeats to master
- Master arbitrates: sends latest member list or explicit suspicion

**ClusterJoinManager** (`hazelcast/internal/cluster/impl/ClusterJoinManager.java`):
- Multi-step join: `handleJoinRequest` -> `ensureNodeIsReady` -> `ensureValidConfiguration` -> `authenticate` -> `validateJoinRequest` -> `startJoin`
- Config compatibility check before admission
- Recently-joined member UUID tracking to prevent duplicate joins
- Stale join prevention with configurable duration

**SplitBrainHandler** (`hazelcast/internal/cluster/impl/SplitBrainHandler.java`):
- Master-only: runs periodically, searches for other clusters via Joiner
- Merge decision based on `SplitBrainMergeCheckResult`: LOCAL_NODE_SHOULD_MERGE, REMOTE_NODE_SHOULD_MERGE, CANNOT_MERGE

**PartitionRuntimeState** (`hazelcast/internal/partition/PartitionRuntimeState.java`):
- Compact encoding: unique replica array + encoded partition table as `int[][]`
- `stamp` for version tracking (monotonic)
- Includes completed and active migrations for state transfer

**MigrationInfo** (`hazelcast/internal/partition/MigrationInfo.java`):
- `MigrationStatus`: ACTIVE, SUCCESS, FAILED
- Tracks source/destination replicas, replica indexes, partition version increments
- UUID per migration for deduplication

**MigrationPlanner** (`hazelcast/internal/partition/impl/MigrationPlanner.java`):
- Plans migrations from current to target replica state
- Key property: never decreases available replica count
- Handles: COPY (fresh), SHIFT UP/DOWN (move between replica indexes), MOVE (change owner)

### TopGun TS (behavioral reference)

**ClusterManager** (`packages/server/src/cluster/ClusterManager.ts`):
- Simple WebSocket mesh with manual peer list or Kubernetes discovery
- HELLO message for identification (no version, no auth)
- Heartbeat via timer + FailureDetector integration
- No membership versioning, no join ceremony

**PartitionService** (`packages/server/src/cluster/PartitionService.ts`):
- `PARTITION_COUNT = 271` (prime, defined in core package)
- Simple modulo assignment: `partitionId = i % allMembers.length`
- Backup assignment: sequential after owner in sorted member list
- Version tracking exists (`mapVersion`) but no per-partition versioning
- `getPartitionMap()` builds full map for client consumption (has known bug: wrong ports for remote nodes)

**FailureDetector** (`packages/server/src/cluster/FailureDetector.ts`):
- Phi-accrual implementation: `calculatePhi()` using interval history statistics
- 3-state detection: normal -> suspected -> confirmed failed
- Configurable: `phiThreshold`, `minSamples`, `maxSamples`, `confirmationTimeoutMs`
- Well-designed but simplified phi formula (uses std-dev ratio rather than full CDF)

**PartitionState enum** (core package):
- `STABLE`, `MIGRATING`, `SYNC`, `FAILED`

### Quickwit chitchat (Rust implementation patterns)

**Cluster** (`quickwit-cluster/src/cluster.rs`):
- Built on `chitchat` crate (scuttlebutt gossip + phi-accrual)
- `Arc<RwLock<InnerCluster>>` for shared cluster state
- Gossip over UDP, gRPC for data plane
- `ClusterChangeStream` (tokio mpsc channel) for reactive membership events

**ClusterChange** (`quickwit-cluster/src/change.rs`):
- `enum ClusterChange { Add(ClusterNode), Update(ClusterNode), Remove(ClusterNode) }`
- `ClusterChangeStreamFactory` trait for testability
- Change detection via sorted key diff of previous vs new node states

**ClusterMember** (`quickwit-cluster/src/member.rs`):
- `NodeId`, `GenerationId` (for distinguishing restarts), enabled services
- `NodeStateExt` trait extends chitchat's `NodeState` with application-specific methods

### TiKV (Rust concurrency patterns)

**Batch System FSM** (`batch-system/src/fsm.rs`):
- `Fsm` trait: `is_stopped()`, `set_mailbox()`, `take_mailbox()`, `get_priority()`
- `FsmState<N>`: three states (NOTIFIED, IDLE, DROP) via `AtomicUsize`
- Lock-free ownership transfer between scheduler and FSM

**Router** (`batch-system/src/router.rs`):
- `normals: Arc<DashMap<u64, BasicMailbox<N>>>` for lock-free partition lookup
- Routes messages to FSM mailboxes by address (region ID in TiKV)
- Separate control FSM for global operations

**BasicMailbox** (`batch-system/src/mailbox.rs`):
- `LooseBoundedSender` for messages + `Arc<FsmState>` for ownership
- `force_send` / `try_send` with automatic FSM scheduling on new message

**RegionState** (`in_memory_engine/src/region_manager.rs`):
- `enum RegionState { Pending, Loading, Active, LoadingCanceled, PendingEvict, Evicting }`
- Each region has `CacheRegionMeta` with state, snapshot list, safe point

### Existing TopGun Rust Code

**server-rust/src/traits.rs**: 6 foundational traits (ServerStorage, MapProvider, SchemaProvider). No cluster/partition traits yet.

**core-rust/src/messages/cluster.rs**: Wire types already defined: `NodeStatus`, `NodeInfo`, `PartitionInfo`, `PartitionMapPayload`, `PartitionMapRequestPayload`, plus distributed search/subscription types.

**core-rust/src/hash.rs**: `fnv1a_hash()` with UTF-16 cross-language compatibility. This is the foundation for `hash_to_partition(key) -> u32`.

## Trade-offs Analysis

| Aspect | Option 1: Full HC Port | Option 2: Gossip-Only | Option 3: Hybrid (recommended) |
|--------|----------------------|---------------------|-------------------------------|
| Membership consistency | Strong (versioned) | Eventual | Strong (versioned) |
| Failure detection | TCP heartbeat | Gossip phi-accrual | Gossip-assisted phi-accrual |
| Migration coordination | 3-phase, master | Uncoordinated | 2-phase, master (CRDT-aware) |
| Split-brain handling | Master merge | None built-in | Master merge + CRDT auto-heal |
| Complexity | High | Low | Medium |
| Rust idiom fit | Poor (Java patterns) | Good | Good |
| Connection overhead | O(n^2) TCP | O(1) UDP gossip | O(n) TCP mesh + UDP gossip |
| Client routing | Master push | Compute locally | Master push (versioned) |
| Scalability ceiling | ~1000 nodes | ~100 nodes | ~100 nodes (sufficient for TopGun) |

## Recommendations

**Recommended approach:** Option 3 (Hybrid: master-centric coordination + gossip-assisted failure detection)

**Reasoning:**
1. **CRDT advantage:** TopGun's CRDTs eliminate the need for strict write locks during migration. A partition can accept writes from both old and new owner simultaneously -- CRDTs merge deterministically. This simplifies Hazelcast's 3-phase migration to 2 phases.
2. **Master coordination is needed:** Partition assignment must be consistent across all nodes at any point in time. Gossip-only introduces a window where nodes disagree on partition ownership, causing split-brain writes that even CRDTs can't resolve cleanly (data lands on wrong partition).
3. **Phi-accrual from TS is portable:** The TS FailureDetector is well-designed and maps cleanly to a Rust trait + struct.
4. **DashMap for partition table:** TiKV's pattern of `DashMap<u64, Metadata>` is perfect for 271 partitions -- lock-free reads with fine-grained write locking per partition.
5. **ClusterChangeStream for reactivity:** Quickwit's pattern of `mpsc::UnboundedSender<ClusterChange>` provides a clean reactive API for components to respond to membership changes.

**Implementation notes:**
- Phase 3 basic: master election via oldest-member (Hazelcast convention), simple and deterministic
- Failure detection runs on all nodes independently, reports to master
- Master decides membership changes, publishes versioned MembersView
- Migration uses CRDT merge: destination receives data, merges with any writes that arrived during migration, then master finalizes ownership
- Client partition map push uses existing `PartitionMapPayload` wire type (already in Rust)

## References

### Codebase References
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/internal/cluster/impl/MembershipManager.java` -- versioned MemberMap, MembersView, atomic swap
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/internal/cluster/impl/ClusterHeartbeatManager.java` -- pluggable failure detection, heartbeat complaint protocol
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/internal/cluster/impl/ClusterJoinManager.java` -- multi-step join ceremony
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/internal/cluster/fd/ClusterFailureDetector.java` -- failure detector trait interface
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/internal/partition/PartitionRuntimeState.java` -- compact partition table encoding
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/internal/partition/MigrationInfo.java` -- migration metadata
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationPlanner.java` -- migration ordering algorithm
- `/Users/koristuvac/Projects/rust/quickwit/quickwit/quickwit-cluster/src/change.rs` -- ClusterChange enum, ClusterChangeStream
- `/Users/koristuvac/Projects/rust/quickwit/quickwit/quickwit-cluster/src/cluster.rs` -- chitchat integration, Arc<RwLock<InnerCluster>>
- `/Users/koristuvac/Projects/rust/tikv/components/batch-system/src/fsm.rs` -- Fsm trait, FsmState
- `/Users/koristuvac/Projects/rust/tikv/components/batch-system/src/router.rs` -- DashMap<u64, BasicMailbox<N>>
- `/Users/koristuvac/Projects/rust/tikv/components/in_memory_engine/src/region_manager.rs` -- RegionState enum
- `/Users/koristuvac/Projects/topgun/topgun/packages/server/src/cluster/ClusterManager.ts` -- TS behavioral reference
- `/Users/koristuvac/Projects/topgun/topgun/packages/server/src/cluster/PartitionService.ts` -- TS partition assignment
- `/Users/koristuvac/Projects/topgun/topgun/packages/server/src/cluster/FailureDetector.ts` -- phi-accrual implementation

### External References
- [Decentralized cluster membership in Rust (Quickwit chitchat)](https://quickwit.io/blog/chitchat)
- [chitchat GitHub repository](https://github.com/quickwit-oss/chitchat)
- [Rust Distributed Systems Ecosystem 2025](https://andrewodendaal.com/rust-distributed-systems-ecosystem/)

### Main Deliverable
- `/Users/koristuvac/Projects/topgun/topgun/.specflow/reference/RUST_CLUSTER_ARCHITECTURE.md`
