---
id: RES-005
topic: Service & Operation Architecture
created: 2026-02-20
status: complete
---

# Research: Service & Operation Architecture for Rust Server

## Summary

TopGun's Rust server should use a **hybrid architecture**: direct async with Tower middleware for the request pipeline (like Quickwit), a typed service registry with lifecycle hooks (inspired by Hazelcast's ServiceManager/ManagedService), and partition-routable operations with provenance tracking (Hazelcast's Operation model adapted to Rust enums). This avoids the overhead of a full actor framework while providing the composability, testability, and migration-awareness that the TS server lacks.

## Background

The TS server uses 26+ stateless message handlers (29 message routes) grouped into 9 domains (CRDT, Sync, Query, Messaging, Coordination, Search, Persistence, Client, Server). These handlers are created by `handlers-module.ts` and routed by a flat `MessageRegistry`. This design has several limitations:

1. **No partition awareness** -- handlers do not know if they own the partition for a given key; routing logic is scattered across individual handlers.
2. **No lifecycle hooks** -- handlers have no init/shutdown/reset semantics; cleanup relies on ad-hoc `LifecycleManager` callbacks.
3. **No migration barriers** -- operations execute during partition migration, leading to stale reads.
4. **No provenance tracking** -- no distinction between client-originated, backup, forwarded, or WAN-replicated operations.
5. **No backpressure** -- `BackpressureRegulator` exists but is not integrated into the handler pipeline.
6. **Known bugs** -- `BatchProcessingHandler.processBatchAsync` nests inter-node forwarded messages incorrectly; `PartitionService.getPartitionMap()` returns wrong ports.

Phase 3 (TODO-065) requires a proper operation execution model that accounts for partition routing, migration barriers, and service lifecycle. This research informs that design.

## Hazelcast Findings

### ServiceManager Pattern

Hazelcast's `ServiceManagerImpl` (`spi/impl/servicemanager/impl/ServiceManagerImpl.java`) is a ConcurrentHashMap-based registry mapping service names to `ServiceInfo` objects. Key features:

- **Name-based registration**: `registerService(String name, Object service)` stores services by string name.
- **Interface-based discovery**: `getServices(Class<S> serviceClass)` finds all services implementing a given interface. This enables cross-cutting concerns like `MigrationAwareService`.
- **Core vs extension services**: Core services (ClusterService, PartitionService) are registered first and placed at the head of lists. They cannot be replaced.
- **Ordered initialization**: `start()` calls `registerServices()` then `initServices()`. Init calls `ManagedService.init()` on each registered service.
- **Reverse-order shutdown**: `shutdown()` reverses the service list so core services stop last.
- **~20 default services** including MapService, TopicService, LockSupportService, ExecutorService, etc.

### ManagedService Lifecycle

`ManagedService` (`internal/services/ManagedService.java`) defines three lifecycle hooks:

```java
void init(NodeEngine nodeEngine, Properties properties);
void reset();  // force-start, split-brain merge
void shutdown(boolean terminate);
```

### MigrationAwareService

`MigrationAwareService` (`internal/partition/MigrationAwareService.java`) adds migration hooks:

```java
Operation prepareReplicationOperation(PartitionReplicationEvent event);
void beforeMigration(PartitionMigrationEvent event);
void commitMigration(PartitionMigrationEvent event);
void rollbackMigration(PartitionMigrationEvent event);
```

This is critical: services that own partition data (MapService, etc.) get notified before/after migration and can prepare replication data. TopGun's TS server has nothing equivalent.

### Operation Model

Hazelcast's `Operation` (`spi/impl/operationservice/Operation.java`) is a rich base class with:

- **Partition ID** (`partitionId`): -1 means generic (not partition-bound), 0+ means partition-specific.
- **Replica index** (`replicaIndex`): Which replica this operation targets (0=primary, 1+=backup).
- **Service name** (`serviceName`): Which service handles this operation.
- **Call ID** (`callId`): Unique ID for request-response correlation.
- **Caller UUID** (`callerUuid`): Provenance tracking.
- **Lifecycle hooks**: `beforeRun()`, `run()` or `call()`, `afterRun()`, `afterRunFinal()`.
- **CallStatus**: `RESPONSE`, `VOID`, `WAIT` (blocking), `OFFLOAD` (async executor).
- **Timeout support**: `callTimeout`, `waitTimeout` for blocking operations.

### OperationRunner

`OperationRunnerImpl` (`spi/impl/operationservice/impl/OperationRunnerImpl.java`) executes operations with precondition checks:

1. `checkNodeState()` -- node must be ACTIVE.
2. `timeout()` -- operation call timeout.
3. `ensureNoPartitionProblems()` -- partition must be owned by this node, NOT migrating (unless ReadonlyOperation + stale reads enabled).
4. `ensureNoSplitBrain()` -- split-brain protection.
5. `op.beforeRun()`.
6. `op.call()` / `op.run()`.
7. Dispatch based on CallStatus (sendBackups, park for wait, offload).
8. `op.afterRun()`.

### BackpressureRegulator

Hazelcast's `BackpressureRegulator` limits concurrent invocations per partition and periodically forces async backups to sync. This prevents operation queue overflow.

### Handler Pipeline (Networking)

Hazelcast's `InboundHandler`/`OutboundHandler` form a composable pipeline for protocol handling (byte decoding, packet framing). These are lower-level than Tower middleware but serve the same purpose.

## Quickwit Findings

### Actor Framework (`quickwit-actors`)

Quickwit implements a custom actor framework with these key abstractions:

- **`Actor` trait**: Defines `ObservableState`, `queue_capacity()`, `initialize()`, `on_drained_messages()`, `finalize()`.
- **`Handler<M>` trait**: `async fn handle(&mut self, message: M, ctx: &ActorContext<Self>) -> Result<Self::Reply, ActorExitStatus>`.
- **`Mailbox<A>`**: Typed handle for sending messages to an actor. Lightweight to clone. Supports high-priority (commands) and low-priority (messages) channels.
- **`Universe`**: Top-level lifecycle manager. Spawns actors, supports `quit()` (graceful), `kill()`, `observe()`. NOT a singleton -- tests get their own universe.
- **`ActorRegistry`**: TypeId-based registry. Discovers actors by type. Supports observation via JSON serialization of `ObservableState`.
- **`SpawnBuilder`**: Configures actor spawn (kill switch, runtime handle).

Key design decisions:
- Observable state for testing/admin UI.
- Heartbeat-based liveness detection (30s default, 500ms in tests).
- Accelerated time for testing.
- Supervisors for actor restart.

### Tower Integration

Quickwit builds custom Tower layers in `quickwit-common/src/tower/`:

- **`LoadShedLayer`**: Semaphore-based in-flight request limiting. Rejects with custom error when overloaded.
- **`EventListenerLayer`**: Publishes events to an `EventBroker` after requests complete.
- **`RetryLayer`**: With configurable `RetryPolicy`.
- **`TimeoutLayer`**: Per-request timeout.
- **`BufferLayer`**: Channel-based buffering.
- **`GrpcMetricsLayer`**: Per-RPC metrics.
- **`CircuitBreakerLayer`**: Circuit breaker pattern.
- **`RateLimitLayer`**: Rate limiting.

### Service Composition

In `quickwit-serve/src/lib.rs`, Quickwit wires services at startup using `ServiceBuilder`:

```rust
let metastore = MetastoreServiceClient::tower()
    .stack_layer(shared_layer)
    .stack_create_index_layer(broker_layer.clone())
    .stack_delete_index_layer(broker_layer.clone())
    .build(metastore);
```

This shows per-RPC layer customization: shared layers apply to all RPCs, then specific layers apply to specific operations. This is directly applicable to TopGun's operation domains.

## TiKV Findings

### Worker/Scheduler Pattern

TiKV's `worker` module (`components/tikv_util/src/worker/`) provides:

- **`Runnable` trait**: `fn run(&mut self, task: Self::Task)`, `fn on_tick()`, `fn shutdown()`.
- **`RunnableWithTimer`**: Adds `on_timeout()` and `get_interval()` for periodic work.
- **`Scheduler<T>`**: Sends tasks via unbounded MPSC channel. Has `pending_capacity` for backpressure (returns `ScheduleError::Full`).
- **`Worker`**: Owns a thread pool (`FuturePool`). Can spawn interval tasks, async tasks.
- **`LazyWorker<T>`**: Deferred start -- create scheduler early, start runner later.

Key design: each subsystem (GC, CDC, PD heartbeat) gets its own `Worker` with a specialized `Runnable` implementation. This provides isolation without actor overhead.

### Server Initialization

TiKV's server initialization is a long sequential process assembling many subsystems. Each subsystem gets its own `Worker` or `LazyWorker`. The pattern is: create schedulers early, wire them together, then start workers in dependency order.

## Databend Findings

### GlobalServices Pattern

Databend's `GlobalServices::init()` (`src/query/service/src/global_services.rs`) uses:

- **`GlobalInstance`**: A `OnceCell` + `TypeMap` singleton registry. Services are stored by type via `GlobalInstance::set(Arc<T>)` and retrieved via `GlobalInstance::get::<T>()`.
- **Explicit initialization order**: Comments warn "The order of initialization is very important". Config first, then logging, then runtimes, then cluster discovery, then catalogs, etc.
- **No dynamic registration**: All services are known at compile time. No runtime service discovery.

This is the simplest pattern but least flexible. Good for initialization but doesn't provide lifecycle hooks or migration awareness.

## TopGun TS Server Analysis

### Handler Domain Mapping (29 routes, 9 domains)

| Domain | Handlers | Routes | Partition-Bound? |
|--------|----------|--------|-----------------|
| CRDT | OperationHandler, BatchProcessingHandler, GCHandler | CLIENT_OP, OP_BATCH | Yes (key-based) |
| Sync | LwwSyncHandler, ORMapSyncHandler | SYNC_INIT, MERKLE_REQ_BUCKET, ORMAP_* (4) | Yes (map-based) |
| Query | QueryHandler, QueryConversionHandler | QUERY_SUB, QUERY_UNSUB | Depends (distributed) |
| Messaging | TopicHandler, BroadcastHandler | TOPIC_SUB/UNSUB/PUB | No (broadcast) |
| Coordination | LockHandler, PartitionHandler | LOCK_REQUEST/RELEASE, PARTITION_MAP_REQUEST | Yes (lock) / No (partition) |
| Search | SearchHandler | SEARCH, SEARCH_SUB/UNSUB | No (distributed) |
| Persistence | JournalHandler, CounterHandler, EntryProcessorAdapter, ResolverHandler | 8 routes | Mixed |
| Client | AuthHandler, WebSocketHandler, ClientMessageHandler | (connection-level) | No |
| Server | HeartbeatHandler, PersistenceHandler, OperationContextHandler, WriteConcernHandler | (internal) | No |

### Key Observations

1. **CRDT and Sync** handlers are partition-bound -- they should be partition-routable operations.
2. **Query and Search** are distributed operations spanning multiple partitions.
3. **Messaging, Client, Server** handlers are generic (not partition-bound).
4. **Coordination** is mixed: locks are partition-bound, partition map requests are generic.
5. The `MessageRegistry` is a flat dispatch table with no middleware pipeline.
6. Dependencies between handlers are wired via closures at construction time (dependency injection by closure).

## Options Explored

### Option A: Full Actor-Based (Quickwit-style)

**Description:** Each handler domain becomes an actor with its own mailbox. Messages are routed to actors via typed mailboxes. Tower layers wrap actor-to-actor communication.

**Pros:**
- Strong isolation between domains
- Observable state for testing/admin
- Natural backpressure via bounded mailboxes
- Familiar to Quickwit/Actix users

**Cons:**
- High overhead for simple request-response patterns
- 29 message types x 9 domains = many message enum variants
- Partition routing requires custom routing layer on top of actor mailboxes
- TopGun operations are mostly request-response, not stream-based -- actors add indirection for no benefit
- Actor restart semantics are complex for stateful partition operations

### Option B: Worker/Scheduler (TiKV-style)

**Description:** Each domain gets a dedicated `Worker` with a task enum. Operations are scheduled via `Scheduler<DomainTask>`. Workers run on dedicated thread pools.

**Pros:**
- Simple and battle-tested in TiKV
- Natural backpressure via bounded channels
- Good isolation between domains
- Easy to add periodic tasks (GC, heartbeat)

**Cons:**
- Not ergonomic for request-response -- need manual oneshot channels for replies
- No middleware composition (each worker handles its own concerns)
- Thread-per-domain can waste resources for low-traffic domains
- No Tower integration -- cross-cutting concerns (metrics, timeout, auth) must be duplicated

### Option C: Direct Async with Tower Middleware

**Description:** Operations are dispatched directly via `tower::Service` implementations. A `ServiceRouter` maps message types to `Service<Request, Response>`. Tower layers provide middleware (timeout, metrics, auth, partition routing, backpressure).

**Pros:**
- Minimal overhead -- direct async dispatch
- Full Tower ecosystem (timeout, retry, load shed, metrics)
- Per-operation middleware customization (like Quickwit's per-RPC layers)
- Natural `async fn` ergonomics in Rust
- Partition routing is a Tower layer, not a separate subsystem

**Cons:**
- `tower::Service` trait is complex (`poll_ready`, `Future` associated type)
- No built-in lifecycle management or observability
- Requires a separate mechanism for service registry and shutdown

### Option D: Hybrid (recommended)

**Description:** Combine:
1. **ServiceRegistry** (Hazelcast-inspired) for lifecycle management and interface-based discovery.
2. **Tower middleware pipeline** (Quickwit-inspired) for request processing with composable layers.
3. **Partition-routable Operation enum** (Hazelcast-inspired) for type-safe operation dispatch.
4. **LazyWorker** (TiKV-inspired) for background tasks (GC, heartbeat, persistence flush).

**Pros:**
- Each pattern is used where it fits best
- ServiceRegistry provides lifecycle hooks without actor overhead
- Tower middleware handles cross-cutting concerns composably
- Background workers handle periodic tasks without blocking the request pipeline
- Operation enum is Rust-native (no Java inheritance hierarchy)

**Cons:**
- More concepts to learn (registry + tower + workers)
- Must define clear boundaries between subsystems

## Trade-offs Analysis

| Aspect | Option A (Actor) | Option B (Worker) | Option C (Direct Async) | Option D (Hybrid) |
|--------|-----------------|-------------------|------------------------|-------------------|
| Complexity | High | Medium | Medium | Medium-High |
| Partition routing | Custom layer needed | Manual | Tower layer | Tower layer |
| Backpressure | Mailbox bounds | Channel bounds | LoadShed layer | LoadShed layer |
| Testability | Good (observable) | Medium | Good (Service trait) | Good |
| Migration awareness | Manual | Manual | Custom layer | ServiceRegistry hooks |
| Lifecycle (init/shutdown) | Universe.quit() | Worker.stop() | Manual | ServiceRegistry |
| Background tasks | Actor timers | Worker intervals | tokio::spawn | LazyWorker |
| Tower integration | Awkward | None | Native | Native |
| Request-response | Overhead | Channel overhead | Zero overhead | Zero overhead |
| Code size | Large | Medium | Small | Medium |

## Codebase Findings

### Existing Rust Code

- `packages/server-rust/src/traits.rs` -- Defines `ServerStorage`, `MapProvider`, `SchemaProvider` traits. These are Phase 1 foundational traits that the service registry should encompass.
- `packages/server-rust/src/lib.rs` -- Currently just re-exports traits. Will become the crate root for the server.
- `packages/core-rust/src/traits.rs` -- Defines `Processor` and `QueryNotifier` traits.
- `packages/core-rust/src/messages/` -- 77-variant `Message` enum. Operations will be derived from these message types.

### TS Server Files

- `packages/server/src/modules/handlers-module.ts` -- 29 message routes across 9 domains. This is the behavioral reference for what operations the Rust server must support.
- `packages/server/src/coordinator/` -- 26+ handler files, each implementing one handler class.
- `packages/server/src/coordinator/message-registry.ts` -- Flat dispatch table from message type to handler function.
- `packages/server/src/modules/lifecycle-module.ts` -- Shutdown hooks (the equivalent of `ManagedService.shutdown()`).

## Recommendations

**Recommended approach:** Option D (Hybrid)

**Reasoning:**
1. TopGun's operations are mostly request-response (CRDT put/get, query, search) -- actors add overhead for no benefit.
2. Partition routing is a cross-cutting concern best handled as Tower middleware, not per-handler logic.
3. Service lifecycle (init/shutdown/migration) needs explicit hooks -- neither pure Tower nor pure actors provide this naturally.
4. Background tasks (GC, heartbeat, persistence flush) need dedicated workers, not request-response dispatch.
5. The Rust type system (enums, traits) replaces Java's inheritance hierarchy -- we don't need actor-style dynamic dispatch.

**Implementation notes:**
1. Define `ServiceRegistry` trait with `register()`, `get::<T>()`, `get_by_name()`, `shutdown()`.
2. Define `ManagedService` trait with `init()`, `reset()`, `shutdown()`.
3. Define `MigrationAwareService` trait with `before_migration()`, `commit_migration()`, `rollback_migration()`, `prepare_replication()`.
4. Define `Operation` enum (not trait) derived from the Message enum -- partition ID, service name, caller provenance.
5. `OperationService` dispatches operations through a Tower middleware stack: Auth -> Timeout -> PartitionRouting -> MigrationBarrier -> Metrics -> Handler.
6. Each domain (CRDT, Sync, Query, etc.) implements `tower::Service<DomainRequest>`.
7. Background tasks (GC, heartbeat) use `LazyWorker`-style dedicated workers.

## References

### Hazelcast (Architecture)
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/spi/impl/servicemanager/ServiceManager.java` -- ServiceManager interface
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/spi/impl/servicemanager/impl/ServiceManagerImpl.java` -- Full implementation with registration, init, shutdown
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/internal/services/ManagedService.java` -- Lifecycle hooks (init, reset, shutdown)
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/internal/partition/MigrationAwareService.java` -- Migration hooks (before, commit, rollback)
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/Operation.java` -- Operation base class (918 lines)
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/OperationRunnerImpl.java` -- Operation execution with preconditions
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/OperationServiceImpl.java` -- Operation dispatch
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/BackpressureRegulator.java` -- Backpressure control
- `/Users/koristuvac/Projects/hazelcast/hazelcast/src/main/java/com/hazelcast/internal/partition/operation/AbstractPartitionOperation.java` -- Migration-aware base operation

### Quickwit (Rust Patterns)
- `/Users/koristuvac/Projects/rust/quickwit/quickwit/quickwit-actors/src/actor.rs` -- Actor + Handler traits
- `/Users/koristuvac/Projects/rust/quickwit/quickwit/quickwit-actors/src/universe.rs` -- Universe lifecycle manager
- `/Users/koristuvac/Projects/rust/quickwit/quickwit/quickwit-actors/src/registry.rs` -- TypeId-based actor registry
- `/Users/koristuvac/Projects/rust/quickwit/quickwit/quickwit-actors/src/mailbox.rs` -- Typed mailbox with priority channels
- `/Users/koristuvac/Projects/rust/quickwit/quickwit/quickwit-common/src/tower/mod.rs` -- Custom Tower layers
- `/Users/koristuvac/Projects/rust/quickwit/quickwit/quickwit-common/src/tower/load_shed.rs` -- Semaphore-based load shedding
- `/Users/koristuvac/Projects/rust/quickwit/quickwit/quickwit-common/src/tower/event_listener.rs` -- Event publishing layer
- `/Users/koristuvac/Projects/rust/quickwit/quickwit/quickwit-serve/src/lib.rs` -- Service composition with per-RPC layers

### TiKV (Worker Pattern)
- `/Users/koristuvac/Projects/rust/tikv/components/tikv_util/src/worker/mod.rs` -- Worker module overview
- `/Users/koristuvac/Projects/rust/tikv/components/tikv_util/src/worker/pool.rs` -- Scheduler, Worker, LazyWorker, Runnable traits
- `/Users/koristuvac/Projects/rust/tikv/components/tikv_util/src/worker/future.rs` -- Async Worker variant

### Databend (Singleton Pattern)
- `/Users/koristuvac/Projects/rust/databend/src/query/service/src/global_services.rs` -- GlobalServices::init() with ordered initialization

### TopGun TS Server (Behavioral Reference)
- `/Users/koristuvac/Projects/topgun/topgun/packages/server/src/modules/handlers-module.ts` -- 29 message routes, 9 domains, handler creation
- `/Users/koristuvac/Projects/topgun/topgun/packages/server/src/coordinator/` -- 26+ handler implementations

### TopGun Rust Server (Existing)
- `/Users/koristuvac/Projects/topgun/topgun/packages/server-rust/src/traits.rs` -- ServerStorage, MapProvider, SchemaProvider
- `/Users/koristuvac/Projects/topgun/topgun/packages/server-rust/src/lib.rs` -- Crate root
- `/Users/koristuvac/Projects/topgun/topgun/packages/core-rust/src/messages/` -- 77-variant Message enum
